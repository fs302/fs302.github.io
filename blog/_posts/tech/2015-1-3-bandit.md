---
layout: post
title: Bandit:一种简单而强大的在线学习算法
category: tech 
description: 介绍了Bandit在线学习算法，以及它的应用。
published: true

---

假设我有5枚硬币，都是正反面不均匀的。我们玩一个游戏，每次你可以选择其中一枚硬币掷出，如果掷出正面，你将得到一百块奖励。掷硬币的次数有限（比如10000次），显然，如果要拿到最多的利益，你要做的就是尽快找出“正面概率最大”的硬币，然后就拿它赚钱了。

这个问题看起来很数学化，其实它在我们的生活中经常遇见。比如我们现在有很多在线场景，遇到一个相同的问题：一个平台这么多信息，该展示什么给用户，才能有最好的收益（比如点击率）？

Google作为最大的搜索广告公司，在用户搜索时该展示什么广告；Facebook作为社交平台，当用户好友过载的时候，该怎么组织好友的说说（把你最感兴趣的放前面）；Taobao有海量的商品池子，该如何捞取用户最容易剁手的商品展示出来？

一切通过数据收集而得到的概率预估任务，都能通过Bandit系列算法来进行在线优化。这里的“在线”，指的不是互联网意义上的线上，而是只算法模型参数根据观察数据不断演变。

Bandit算法的创造其实来源于人类的经验，这个算法框架包含两个部分，一是探索未知（explore），二是利用已知（exploit）。一部分精力做探索（不考虑曾经的经验），一部分精力做采集（利用已知的最好策略）。

## How Bandit

首先来看看Bandit的概率原理，我们希望知道每一个硬币“正面”的概率 $p_i$ 。事实上我们能观察到的，只是这个硬币正面的频率

$$ \mu_i = \frac{正面次数}{全部尝试次数}$$

怎么利用起观察到的频率，来最好地预估真实的概率呢？下面介绍4种策略，分别是随机（Random）、简单观察（Naive）、ε-贪心法（ε-Greedy）、置信上限法（UCB）。

#### Random

每次随机选择一枚硬币进行投掷。如果不能胜过这个策略，就不必玩了。

#### Naive

先给每个硬币一定次数的尝试，比如每个硬币掷10次，根据每个硬币正面朝上的次数，选择正面频率最高的那个硬币，作为最佳策略。这也是大多人能想到的方法。

但是这个策略有几个明显问题：

1. 10次尝试真的靠谱吗？最差的硬币也有可能在这10次内有高于最好硬币的正面次数。
2. 假设你选到的这个硬币在投掷次数多了后发生了问题（比如掉屑），改变了其属性，导致其正面的概率大大降低，如果你还死守着它，那不是吃大亏了？（这是对变量的考虑）
3. 就算你给一个硬币10次机会，如果硬币真的很多，比如$K>100$，给每个硬币10次机会是不是也太浪费了呢？等所有硬币都尝试过，再回来“赚钱”，花儿都谢了！

#### ε-Greedy

有了前两个垫背，可以开始让Bandit登场了。ε-Greedy就是一种很机智的Bandit算法：它让每次机会以ε的概率去“探索”，1-ε的概率来“开发”。也即，如果一次机会落入ε中，则随机选择一个硬币来投掷，否则就选择先前探索到正面概率最大的硬币。这个策略有两个好处：

1. 它能够应对变化，如果硬币“变质”了，它也能及时改变策略。
2. ε-Greedy机制让玩的过程更有趣，有时“探索”，有时“赚钱”。

在此基础上，又能引申出很多值得研究的问题，比如ε应该如何设定呢？它应不应该随着时间而变？因为随着探索次数的增多，好的选择自然浮现得比较明显了。ε大则使得模型有更大的灵活性（能更快的探索到未知，适应变化），ε小则会有更好的稳定性（有更多机会去“开发”）。

#### UCB

在统计学中，对于一个未知量的估计，总能找到一种量化其置信度的方法。最普遍的分布正态分布（或曰高斯分布）$N(\mu ,\delta )$，其中的E就是估计量的期望，而$\delta$则表示其不确定性（$\delta$越大则表示越不可信）。比如你掷一个标准的6面色子，它的平均值是3.5，而如果你只掷一次，比如说到2，那你对平均值的估计只能是2，但是这个置信度应该很低，我们可以知道，这个色子的预估平均值是2，而以95%的置信区间在[1.4,5.2]。

UCB（Upper Confidence Bound - 置信上限）就是以均值的置信上限为来代表它的预估值：

$$ \widehat{\mu_i } = \widehat{\mu_i} + 2\sqrt{\frac{1}{n_i}} $$

上面是一个例子，其中$\mu_i$是对期望的预估，$n_i$是尝试次数，可以看到对$i$的尝试越多，其预估值与置信上限的差值就越小。也就是越有置信度。

这个策略的好处是，能让没有机会尝试的硬币得到更多尝试的机会，是骡子是马拉出来溜溜！将整个探索+开发的过程融合到一个公式里面，很完美！

#### 模拟结果

将这几个策略做一下模拟，取K=5个硬币，每次10000轮投掷机会，跑100次取平均。得到结果如下：

1. 随机：每次随机取一枚硬币投掷
2. 简单观察：先给每个硬币100次机会，然后以正面概率最大的硬币为策略。
3. ε-Greedy：取$\epsilon=0.01$进行探索，$1-ε$进行开发。
4. UCB1：以$(1 - 1/t)$的上限进行探索
5. UCB-95%：取95%的置信区间进行探索

![bandit_simulation](http://findshine.qiniudn.com/figure.png)

上图以累积后悔（Cumulative Expected Regret）来作为评估指标，横坐标是投掷次序，纵坐标是累积后悔（取对数）。后悔最小的算法最好。Regret定义如下：

$$ R_T = \sum\_{i=1}^{T} (w\_{opt} - w_{B(i)}) $$

可以看出，随机的效果最烂，Naive算法在前K*100轮跟随机效果一样烂（因为在收集数据，没有开始利用）。ε-Greedy的收敛效果好，但因为有那ε的浪费，到最后还是跟Naive一样浪费了很多机会。UCB的表现最好，收敛快、花费小！

这里只是模拟了固定概率下这些算法的表现，如果预估量（正面概率）是一个会变的量，这些算法的表现会重新洗牌吗？后续可以探索下！

## Bandit application

说了这么多掷硬币，这个算法在真实世界有什么大展身手的地方呢？小列一些：

* 在线排序（Online Ranking）
	* CTR预估
* Stock Option
	* 选择最好的股票进行投资
* A/B test
	* 快速选择好的AB版本，快速淘汰差的

附1：参考链接

* [Bandits for Recommendation Systems](http://engineering.richrelevance.com/bandits-recommendation-systems/)
* [Bayesian Methods for Hackers - Chapter6](http://nbviewer.ipython.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter6_Priorities/Priors.ipynb)
